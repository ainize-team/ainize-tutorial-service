apiVersion: batch/v1
kind: Job
metadata:
  name: lm-evaluation-harness-$(REQUEST_KEY)
  namespace: gpu-product
  labels:
    app: lm-evaluation-harness-$(REQUEST_KEY)
spec:
  template:
    metadata:
      labels:
        app: lm-evaluation-harness-$(REQUEST_KEY)
    spec:
      volumes:
      - name: lm-evaluation-config
        configMap:
          name: lm-evaluation-config
      containers:
      - name: lm-evaluation-harness-$(REQUEST_KEY)
        image: 10.152.183.152/ainize/lm-evaluation-harness:latest
        imagePullPolicy: Always
        resources:
        #requests:
            #memory: 16Gi
            #cpu: 16000m
            #nvidia.com/gpu: "1"
          limits:
            memory: 768Gi
            cpu: 16000m
            nvidia.com/gpu: 8
        command: ["accelerate", "launch", "--config_file", "/tmp/accelerate_config.yaml", "-m", "lm_eval", $(PARAMS)]
        ports:
        - containerPort: 80
        volumeMounts:
        - name: lm-evaluation-config
          mountPath: /tmp
      nodeSelector:
        node-role.kubernetes.io/gpu-worker: gpu-worker
      restartPolicy: Never
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-A100-SXM4-40GB
  backoffLimit: 0
